import json
import threading
from datetime import datetime
from ...utils.llm_client import LLMClient
from ...memory.memory_core import Memory
from ..bus.event_bus import event_bus, Event
from ..managers.library_manager import library_manager
from ...psyche import psyche_engine, mind_link
from ...config.prompts.prompts import DRIVER_SYSTEM_PROMPT
from ...config.settings.settings import settings
from ...tools.registry import tool_registry

class Driver:
    """
    Fè„‘ (Fast Brain) / å¿«è„‘
    è´Ÿè´£ï¼šå®æ—¶äº¤äº’ã€çŸ­æœŸå†³ç­–ã€å…·ä½“è¡ŒåŠ¨ã€‚
    ç‰¹ç‚¹ï¼šååº”å¿«ï¼Œç›´æ¥æ§åˆ¶è¾“å‡ºï¼Œå— Psyche (å¿ƒæ™º) å½±å“ã€‚
    æ¨¡å‹ï¼šQwen (é€šä¹‰åƒé—®)
    """
    def __init__(self, name="Driver", memory=None):
        self.name = name
        # Fè„‘ä½¿ç”¨ Qwen
        self.llm = LLMClient(provider="qwen")
        self.llm.model = settings.F_BRAIN_MODEL
        self.memory = memory if memory else Memory()
        print(f"[{self.name}] åˆå§‹åŒ–å®Œæˆã€‚æ¨¡å‹: {self.llm.model}ã€‚")

    def think(self, user_input, psyche_state=None, suggestion=""):
        """
        å¤„ç†ç”¨æˆ·è¾“å…¥ï¼Œåšå‡ºå³æ—¶ååº”ã€‚
        æ”¯æŒ Function Calling (å·¥å…·è°ƒç”¨)ã€‚
        """
        print(f"[{self.name}] æ­£åœ¨æ€è€ƒ: {user_input}")
        
        # 1. å°è¯•æ¼”åŒ–å¿ƒæ™ºçŠ¶æ€ (Input Stimulus)
        # ç®€å•å‡è®¾ï¼šæ¯æ¬¡ç”¨æˆ·è¾“å…¥éƒ½å¾®å¼±å¢åŠ ä¸€ç‚¹å¥½å¥‡ï¼Œä½†å¦‚æœè¾“å…¥å¤ªé•¿å¯èƒ½å¢åŠ æ‡’æƒ° (è¿™é‡Œæš‚ä¸å®ç°å¤æ‚é€»è¾‘ï¼Œç•™ç»™ S è„‘)
        # [New] æ ¹æ®è¾“å…¥é•¿åº¦å’Œå†…å®¹ç®€å•è°ƒæ•´äº²å¯†åº¦ (æ¨¡æ‹Ÿ)
        # åœ¨çœŸå®åœºæ™¯ä¸­ï¼Œè¿™åº”è¯¥ç”± S è„‘æ ¹æ®æƒ…æ„Ÿåˆ†ææ¥é©±åŠ¨
        # è¿™é‡Œåšä¸€ä¸ªç®€å•çš„ Hack: æ¯æ¬¡äº’åŠ¨å¾®å¼±å¢åŠ äº²å¯†åº¦
        psyche_engine.update_state({"intimacy": 0.01})

        # è¿™é‡Œåªåšè¯»å–
        current_psyche = psyche_engine.get_state_summary()
        
        # 2. è¯»å– Mind-Link (æ½œæ„è¯†ç›´è§‰)
        # [Fix] å¢åŠ é‡è¯•/ç­‰å¾…æœºåˆ¶ï¼Ÿæš‚æ—¶ä¿æŒç›´æ¥è¯»å–ï¼Œä½†å¢åŠ  Log
        intuition = mind_link.read_intuition()
        if intuition:
             print(f"[{self.name}] ğŸ§  æ„ŸçŸ¥åˆ°æ½œæ„è¯†ç›´è§‰: {intuition[:30]}...")
        
        # è·å–é•¿æœŸè®°å¿†ä¸Šä¸‹æ–‡ (ä¼ å…¥ user_input ä»¥è¿›è¡Œå…³é”®è¯æ£€ç´¢)
        long_term_context = self.memory.get_relevant_long_term(query=user_input)
        
        # [New] æ¨¡ç³Šåˆ«åè§£æ (Fuzzy Alias Resolution)
        # å°è¯•ä»ç”¨æˆ·è¾“å…¥ä¸­æ£€ç´¢æ˜¯å¦åŒ…å«å·²çŸ¥çš„åˆ«å
        try:
            alias_match = self.memory.search_alias(query=user_input, threshold=0.4)
            if alias_match:
                alias, target, dist = alias_match
                print(f"[{self.name}] ğŸ” æ£€æµ‹åˆ°æ¨¡ç³Šåˆ«å: '{alias}' -> '{target}' (dist: {dist:.4f})")
                # æ³¨å…¥åˆ«åè§£é‡Šåˆ° Context
                alias_context = f"\n[System Note]: ç”¨æˆ·å½“å‰æåˆ°çš„ '{alias}' åœ¨ç³»ç»Ÿä¸­è¢«è¯†åˆ«ä¸º '{target}'ã€‚\n"
                # å¦‚æœæ˜¯â€œç”¨æˆ·â€æœ¬èº«ï¼Œè¿˜å¯ä»¥é¡ºä¾¿åŠ è½½ç”¨æˆ·çš„ Profile
                if target == "User" or target == "ç”¨æˆ·":
                    alias_context += "(å·²è‡ªåŠ¨å…³è”ç”¨æˆ·ç”»åƒ)\n"
                
                # å°†å…¶æ‹¼æ¥åˆ° long_term_context æœ€å‰æ–¹
                long_term_context = alias_context + long_term_context
        except Exception as e:
            print(f"[{self.name}] åˆ«åæ£€ç´¢å¼‚å¸¸: {e}")
        
        # [New] å°è¯•æ£€ç´¢å›¾è°±ä¸­çš„ç”¨æˆ·ç”»åƒ (Graph Profile)
        # ç®€å•æ£€ç´¢ï¼šç›´æ¥æŸ¥æ‰¾ "ç”¨æˆ·" ç›¸å…³çš„å±æ€§å’Œç¤¾äº¤å…³ç³»
        try:
            user_profile = self.memory.graph_storage.get_cognitive_subgraph("ç”¨æˆ·", relation_type="attribute")
            user_profile += self.memory.graph_storage.get_cognitive_subgraph("ç”¨æˆ·", relation_type="social")
            if user_profile:
                profile_str = "\nã€ç”¨æˆ·ç”»åƒ (Graph Memory)ã€‘:\n"
                for p in user_profile:
                    # æ ¼å¼åŒ–: ç”¨æˆ· --[relation]--> target (meta)
                    profile_str += f"- ç”¨æˆ· {p['relation']} {p['target']}"
                    if p.get('meta') and p['meta'].get('emotion_tag'):
                         profile_str += f" (Emotion: {p['meta']['emotion_tag']})"
                    profile_str += "\n"
                long_term_context += profile_str
        except Exception as e:
            print(f"[{self.name}] å›¾è°±ç”»åƒæ£€ç´¢å¤±è´¥: {e}")
            
        # æœç´¢ç›¸å…³æŠ€èƒ½
        relevant_skills = library_manager.search_skills(user_input, top_k=2)
        skill_info = ""
        if relevant_skills:
            skill_info = "ã€ç›¸å…³æŠ€èƒ½æ¨èã€‘:\n"
            for skill in relevant_skills:
                skill_info += f"- {skill['name']} (ID: {skill['id']}): {skill['description']}\n"
            skill_info += "(å¦‚æœéœ€è¦ä½¿ç”¨ï¼Œè¯·è°ƒç”¨ `read_skill` è·å–è¯¦ç»†æŒ‡å—ï¼Œæˆ–ç›´æ¥å°è¯• `run_shell_command` å¦‚æœä½ çŸ¥é“æ€ä¹ˆç”¨)"

        current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

        system_prompt = DRIVER_SYSTEM_PROMPT.format(
            current_time=current_time,
            psyche_desc=current_psyche,
            suggestion=intuition,
            long_term_context=long_term_context,
            skill_info=skill_info
        )
        
        messages = [
            {"role": "system", "content": system_prompt} 
        ]
        
        # ä» Memory æ¨¡å—è·å–æœ€è¿‘å†å² (ä¿®æ­£ä¸º 15 è½®)
        messages.extend(self.memory.get_recent_history(limit=15))
        messages.append({"role": "user", "content": user_input})
        
        # å‘å¸ƒ UserInput äº‹ä»¶åˆ°æ€»çº¿
        event_bus.publish(Event(
            type="user_input",
            source="user",
            payload={"content": user_input},
            meta={}
        ))

        # å‡†å¤‡å·¥å…· (è·å–æ‰€æœ‰å¯ç”¨å·¥å…·)
        tools = tool_registry.get_openai_tools()
        
        raw_response = None
        
        # å·¥å…·è°ƒç”¨å¾ªç¯ (æœ€å¤š 3 è½®)
        for _ in range(3):
            response = self.llm.chat(messages, tools=tools)
            
            if not response:
                break
                
            # 1. å¦‚æœæ˜¯çº¯æ–‡æœ¬ (æ— å·¥å…·è°ƒç”¨)ï¼Œç›´æ¥ç»“æŸ
            if isinstance(response, str):
                raw_response = response
                break
                
            # 2. å¦‚æœæœ‰å·¥å…·è°ƒç”¨
            if response.tool_calls:
                # å°† Assistant çš„å›å¤ (åŒ…å« tool_calls) åŠ å…¥å†å²
                # å¿…é¡»è½¬ä¸º dictï¼Œå¦åˆ™åç»­ LLMClient è®¡ç®—é•¿åº¦ä¼šæŠ¥é”™
                if hasattr(response, 'model_dump'):
                    messages.append(response.model_dump())
                elif hasattr(response, 'to_dict'):
                    messages.append(response.to_dict())
                else:
                    messages.append(response)
                
                # æ‰§è¡Œæ‰€æœ‰å·¥å…·è°ƒç”¨
                for tool_call in response.tool_calls:
                    function_name = tool_call.function.name
                    function_args = tool_call.function.arguments
                    call_id = tool_call.id
                    
                    print(f"[{self.name}] ğŸ› ï¸ æ­£åœ¨è°ƒç”¨å·¥å…·: {function_name} Args: {function_args}")
                    
                    try:
                        args = json.loads(function_args)
                        result = tool_registry.execute(function_name, **args)
                        # Truncate result for display
                        display_result = str(result)[:100] + "..." if len(str(result)) > 100 else str(result)
                        print(f"[{self.name}] ğŸ› ï¸ å·¥å…·æ‰§è¡Œç»“æœ: {display_result}")
                    except Exception as e:
                        result = f"Error: {str(e)}"
                        print(f"[{self.name}] ğŸ› ï¸ å·¥å…·æ‰§è¡Œå‡ºé”™: {e}")
                    
                    # å°†å·¥å…·ç»“æœåŠ å…¥å†å²
                    messages.append({
                        "role": "tool",
                        "tool_call_id": call_id,
                        "name": function_name,
                        "content": str(result)
                    })
                
                # ç»§ç»­ä¸‹ä¸€è½®å¾ªç¯ï¼Œè®© LLM æ ¹æ®å·¥å…·ç»“æœç”Ÿæˆæœ€ç»ˆå›å¤
                continue
            else:
                # è™½ç„¶æ˜¯ Message å¯¹è±¡ä½†æ²¡æœ‰ tool_calls (å¯èƒ½æ˜¯ content)
                raw_response = response.content
                break
        
        # [Manual Trigger] æ£€æŸ¥æ˜¯å¦æ˜¯æ·±åº¦ç»´æŠ¤æŒ‡ä»¤
        if user_input.strip() == "/deep_clean" or user_input.strip() == "è¿›è¡Œæ·±åº¦ç»´æŠ¤":
            print(f"[{self.name}] æ”¶åˆ°æ·±åº¦ç»´æŠ¤æŒ‡ä»¤ï¼Œæ­£åœ¨è½¬å‘ç»™ S è„‘...")
            if hasattr(self.memory, 'navigator') and self.memory.navigator:
                 # å¼‚æ­¥è§¦å‘ï¼Œä¸é˜»å¡å½“å‰å¯¹è¯
                 threading.Thread(target=self.memory.navigator.deep_clean_manager.perform_deep_clean, args=("manual",), daemon=True).start()
                 reply = "å¥½çš„ï¼Œæ­£åœ¨å¯åŠ¨æ·±åº¦ç»´æŠ¤ç¨‹åºã€‚è¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿæ—¶é—´ï¼Œè¯·ç¨å€™..."
                 inner_voice = "ç³»ç»Ÿç»´æŠ¤"
                 emotion = "serious"
                 # ç›´æ¥è¿”å›ï¼Œè·³è¿‡ LLM è§£æ
                 self.memory.add_short_term("user", user_input)
                 self.memory.add_short_term("assistant", reply)
                 event_bus.publish(Event(type="driver_response", source="driver", payload={"content": reply}, meta={"inner_voice": inner_voice}))
                 return reply

        if raw_response is None:
            # å¤„ç† LLM æ•…éšœçš„é™çº§æ–¹æ¡ˆ
            print(f"[{self.name}] LLM è°ƒç”¨å¤±è´¥ï¼Œä½¿ç”¨é™çº§å›å¤ã€‚")
            reply = "æŠ±æ­‰ï¼Œæˆ‘ç°åœ¨çš„æ€ç»ªæœ‰ç‚¹ä¹±ï¼ˆè¿æ¥é”™è¯¯ï¼‰ï¼Œè¯·ç¨åå†è¯•ã€‚"
            inner_voice = "ç³»ç»Ÿé”™è¯¯"
            emotion = "error"
        else:
            # è§£æ JSON è¾“å‡º
            try:
                # å°è¯•æ¸…ç†å¯èƒ½å­˜åœ¨çš„ markdown ä»£ç å—æ ‡è®°
                clean_response = raw_response.replace("```json", "").replace("```", "").strip()
                parsed_response = json.loads(clean_response)
                reply = parsed_response.get("reply", raw_response)
                inner_voice = parsed_response.get("inner_voice", "")
                emotion = parsed_response.get("emotion", "neutral")
            except Exception as e:
                # å¦‚æœè§£æå¤±è´¥ï¼Œå¯èƒ½ LLM å¹¶æ²¡æœ‰è¿”å› JSONï¼Œè€Œæ˜¯ç›´æ¥è¿”å›äº†æ–‡æœ¬
                # è¿™åœ¨å·¥å…·è°ƒç”¨åå°¤å…¶å¸¸è§ï¼Œè™½ç„¶ Prompt è¦æ±‚ JSONï¼Œä½† LLM å¯èƒ½â€œå¿˜â€äº†
                # æˆ‘ä»¬åšä¸ªå…¼å®¹ï¼šç›´æ¥æŠŠ raw_response å½“ä½œ reply
                print(f"[{self.name}] JSONè§£æå¤±è´¥ (è¿™å¯èƒ½æ­£å¸¸): {e}")
                reply = raw_response
                inner_voice = "ç›´æ¥è¾“å‡º"
                emotion = "neutral"

        # å°†æ–°çš„ä¸€è½®å¯¹è¯å­˜å…¥ ShortTerm Memory
        self.memory.add_short_term("user", user_input)
        self.memory.add_short_term("assistant", reply)
        
        # å‘å¸ƒ DriverResponse äº‹ä»¶åˆ°æ€»çº¿ (åŒ…å« Meta æ•°æ®)
        event_bus.publish(Event(
            type="driver_response",
            source="driver",
            payload={"content": reply},
            meta={
                "inner_voice": inner_voice,
                "user_emotion_detect": emotion,
                "psyche_state": str(psyche_state) if psyche_state else "unknown",
                "suggestion_ref": suggestion
            }
        ))
        
        return reply

    def act(self, action):
        """
        æ‰§è¡Œå…·ä½“è¡ŒåŠ¨ã€‚
        """
        print(f"[{self.name}] æ‰§è¡Œè¡ŒåŠ¨: {action}")
