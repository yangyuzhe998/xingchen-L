

import asyncio
from src.tools.registry import ToolRegistry, ToolTier
from src.utils.logger import logger

# Check for crawl4ai
try:
    from crawl4ai import AsyncWebCrawler
    CRAWL4AI_AVAILABLE = True
except ImportError:
    CRAWL4AI_AVAILABLE = False
    logger.warning("[WebTools] crawl4ai not installed. 'web_crawl' tool will be disabled.")

# Check for duckduckgo_search
try:
    from duckduckgo_search import DDGS
    DDGS_AVAILABLE = True
except ImportError:
    DDGS_AVAILABLE = False
    logger.warning("[WebTools] duckduckgo-search not installed. 'web_search' tool will be disabled.")

@ToolRegistry.register(
    name="web_search",
    description="[å›½å†…å¯ç”¨/æœ¬åœ°ç‰ˆ] ä½¿ç”¨ DuckDuckGo è¿›è¡Œç½‘ç»œæœç´¢ã€‚æ”¯æŒè·å–æœç´¢ç»“æœçš„æ ‡é¢˜ã€é“¾æ¥å’Œæ‘˜è¦ã€‚",
    tier=ToolTier.SLOW,
    schema={
        "type": "object",
        "properties": {
            "query": {"type": "string", "description": "æœç´¢å…³é”®è¯"},
            "max_results": {"type": "integer", "description": "æœ€å¤§ç»“æœæ•°é‡ (é»˜è®¤ 5)", "default": 5}
        },
        "required": ["query"]
    }
)
def web_search(query: str, max_results: int = 5):
    """
    ä½¿ç”¨ DuckDuckGo æœç´¢
    """
    if not DDGS_AVAILABLE:
        return "Error: ç¼ºå°‘ä¾èµ– 'duckduckgo-search'ã€‚è¯·è¿è¡Œ `pip install duckduckgo-search`ã€‚"
    
    try:
        logger.info(f"[WebTools] Searching for: {query} (Max: {max_results})")
        results = DDGS().text(query, max_results=max_results)
        if not results:
            logger.info(f"[WebTools] No results found for: {query}")
            return "No results found."
            
        formatted_results = ""
        for i, r in enumerate(results):
            formatted_results += f"{i+1}. [{r['title']}]({r['href']})\n   {r['body']}\n\n"
        
        logger.info(f"[WebTools] Search completed. Found {len(results)} results.")
        return formatted_results
    except Exception as e:
        logger.error(f"[WebTools] Search failed: {e}", exc_info=True)
        return f"Search Error: {str(e)}"

import os
import hashlib
from datetime import datetime
from src.config.settings.settings import settings


def _run_coro_sync(coro, timeout: float = 60.0):
    """åœ¨åŒæ­¥å‡½æ•°ä¸­å®‰å…¨è¿è¡Œ coroutineã€‚

    - è‹¥å½“å‰çº¿ç¨‹æ²¡æœ‰è¿è¡Œä¸­çš„ event loopï¼šä½¿ç”¨ asyncio.run
    - è‹¥å·²æœ‰è¿è¡Œä¸­çš„ event loopï¼ˆå…¸å‹ï¼šWeb/Uvicorn ç¯å¢ƒï¼‰ï¼šåœ¨æ–°çº¿ç¨‹ä¸­å¯åŠ¨ç‹¬ç«‹ loop æ‰§è¡Œ

    è¿”å› coroutine çš„ç»“æœï¼›å¼‚å¸¸ä¼šå‘å¤–æŠ›å‡ºã€‚
    """
    try:
        asyncio.get_running_loop()
    except RuntimeError:
        return asyncio.run(coro)

    import threading

    result_container = {"result": None, "error": None}

    def runner():
        try:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            result_container["result"] = loop.run_until_complete(coro)
        except Exception as e:
            result_container["error"] = e
        finally:
            try:
                loop.close()
            except Exception:
                pass

    t = threading.Thread(target=runner, daemon=True)
    t.start()
    t.join(timeout=timeout)

    if t.is_alive():
        raise TimeoutError(f"Coroutine did not finish within {timeout}s")
    if result_container["error"] is not None:
        raise result_container["error"]

    return result_container["result"]


@ToolRegistry.register(
    name="web_crawl",
    description="[å›½å†…å¯ç”¨/æœ¬åœ°ç‰ˆ] ä½¿ç”¨ Crawl4AI (Playwright) æŠ“å–ç½‘é¡µå†…å®¹ã€‚è‡ªåŠ¨ä¿å­˜åˆ°çŸ¥è¯†åº“ä¸´æ—¶åŒºã€‚é€‚åˆè¯»å–é•¿æ–‡ç« ã€‚",
    tier=ToolTier.SLOW,
    schema={
        "type": "object",
        "properties": {
            "url": {"type": "string", "description": "è¦æŠ“å–çš„ç½‘é¡µ URL"},
            "bypass_cache": {"type": "boolean", "description": "æ˜¯å¦å¼ºåˆ¶åˆ·æ–°ç¼“å­˜ (é»˜è®¤ False)", "default": False}
        },
        "required": ["url"]
    }
)
def web_crawl(url: str, bypass_cache: bool = False):
    """
    åŒæ­¥åŒ…è£…å™¨ï¼Œè°ƒç”¨å¼‚æ­¥çˆ¬è™«ï¼Œå¹¶å°†ç»“æœä¿å­˜åˆ°æ–‡ä»¶
    """
    if not CRAWL4AI_AVAILABLE:
        return "Error: ç¼ºå°‘ä¾èµ– 'crawl4ai'ã€‚è¯·è¿è¡Œ `pip install crawl4ai` å’Œ `playwright install`ã€‚"

    # [Fix] URL æ¸…æ´—é€»è¾‘
    # ç§»é™¤å¯èƒ½çš„ Markdown é“¾æ¥æ ¼å¼ (e.g. [title](url))
    url = url.strip()
    if url.startswith("[") and "](" in url and url.endswith(")"):
        # æå–æ‹¬å·å†…çš„ URL
        try:
            url = url.split("](")[1][:-1]
        except:
            pass
            
    # ç¡®ä¿ URL ä»¥ http å¼€å¤´
    if not url.startswith("http"):
        # ç®€å•çš„å®¹é”™
        if url.startswith("www"):
            url = "https://" + url
        else:
             return f"Error: Invalid URL format: {url}. URL must start with http:// or https://"

    async def _crawl():
        async with AsyncWebCrawler(verbose=True) as crawler:
            result = await crawler.arun(url=url, bypass_cache=bypass_cache)
            if result.success:
                return result.markdown
            else:
                return f"Error: {result.error_message}"

    try:
        logger.info(f"[WebTools] Crawling URL: {url} (Bypass Cache: {bypass_cache})")
        content = _run_coro_sync(_crawl(), timeout=60)
        if content.startswith("Error"):
            logger.error(f"[WebTools] Crawl internal error: {content}")
            return content
            
        # [New] æ–‡æ¡£è½åœ° (Staging)
        # å°†æŠ“å–çš„å†…å®¹ä¿å­˜åˆ° e:\xingchen-V\storage\knowledge_staging
        staging_dir = os.path.join(settings.PROJECT_ROOT, "storage", "knowledge_staging")
        os.makedirs(staging_dir, exist_ok=True)
        
        # ç”Ÿæˆæ–‡ä»¶å (Hash + Timestamp)
        url_hash = hashlib.md5(url.encode()).hexdigest()[:8]
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"crawl_{timestamp}_{url_hash}.md"
        filepath = os.path.join(staging_dir, filename)
        
        # å†™å…¥æ–‡ä»¶
        with open(filepath, "w", encoding="utf-8") as f:
            f.write(f"# Source: {url}\n")
            f.write(f"# Date: {timestamp}\n\n")
            f.write(content)
            
        logger.info(f"[WebTools] Crawl successful. Saved to {filepath} ({len(content)} chars).")
        
        # è¿”å›æ‘˜è¦å’Œè·¯å¾„
        summary = content[:500] + "..." if len(content) > 500 else content
        return (
            f"âœ… æŠ“å–æˆåŠŸã€‚\n"
            f"ğŸ“ åŸå§‹æ–‡æ¡£å·²ä¿å­˜è‡³: {filepath}\n"
            f"ğŸ“„ å†…å®¹æ‘˜è¦ (å‰500å­—):\n"
            f"{summary}\n\n"
            f"(S-Brain è¯·æ³¨æ„ï¼šå®Œæ•´å†…å®¹å·²å½’æ¡£ï¼Œè¯·åœ¨åæ€å‘¨æœŸä¸­è¯»å–æ­¤æ–‡ä»¶è¿›è¡Œå†…åŒ–ã€‚)"
        )
            
    except Exception as e:
        logger.error(f"[WebTools] Crawl failed: {e}", exc_info=True)
        return f"Exception: {str(e)}"
