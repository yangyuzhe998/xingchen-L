

import asyncio
import requests
from src.tools.registry import ToolRegistry, ToolTier
from src.utils.logger import logger

# Check for crawl4ai
try:
    from crawl4ai import AsyncWebCrawler
    CRAWL4AI_AVAILABLE = True
except ImportError:
    CRAWL4AI_AVAILABLE = False
    logger.warning("[WebTools] crawl4ai not installed. 'web_crawl' tool will be disabled.")

# Check for duckduckgo_search
try:
    from duckduckgo_search import DDGS
    DDGS_AVAILABLE = True
except ImportError:
    DDGS_AVAILABLE = False
    logger.info("[WebTools] duckduckgo-search not available, using domestic search engines.")

# Check for BeautifulSoup
try:
    from bs4 import BeautifulSoup
    BS4_AVAILABLE = True
except ImportError:
    BS4_AVAILABLE = False
    logger.warning("[WebTools] beautifulsoup4 not installed. Domestic search will be limited.")

_HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
                   "(KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
}


def _sogou_search(query: str, max_results: int = 5) -> str:
    """ä½¿ç”¨æœç‹—æœç´¢ (å›½å†…ç›´è¿ï¼Œæ— éœ€ API Key)"""
    if not BS4_AVAILABLE:
        return None
    try:
        url = f"https://www.sogou.com/web?query={requests.utils.quote(query)}"
        resp = requests.get(url, headers=_HEADERS, timeout=10)
        resp.raise_for_status()

        soup = BeautifulSoup(resp.text, "html.parser")
        results = []

        for item in soup.select("div.vrwrap"):
            a = item.select_one("h3 a") or item.select_one("a")
            # æœç‹—çš„æ‘˜è¦åœ¨ p.star-wiki æˆ– div.space-txt æˆ– p æ ‡ç­¾å†…
            snippet = item.select_one("p.star-wiki") or item.select_one("div.space-txt") or item.select_one("p")

            if a:
                title = a.get_text(strip=True)
                href = a.get("href", "")
                body = snippet.get_text(strip=True) if snippet else ""
                results.append({"title": title, "href": href, "body": body})

            if len(results) >= max_results:
                break

        if not results:
            return None

        formatted = ""
        for i, r in enumerate(results):
            formatted += f"{i+1}. [{r['title']}]({r['href']})\n   {r['body']}\n\n"
        return formatted

    except Exception as e:
        logger.warning(f"[WebTools] Sogou search failed: {e}")
        return None


def _baidu_search(query: str, max_results: int = 5) -> str:
    """ä½¿ç”¨ç™¾åº¦æœç´¢ (å›½å†…ç›´è¿å¤‡é€‰)"""
    if not BS4_AVAILABLE:
        return None
    try:
        url = f"https://www.baidu.com/s?wd={requests.utils.quote(query)}&rn={max_results}"
        resp = requests.get(url, headers=_HEADERS, timeout=10)
        resp.raise_for_status()

        soup = BeautifulSoup(resp.text, "html.parser")
        results = []

        for item in soup.select("div.c-container"):
            a = item.select_one("h3 a")
            snippet = item.select_one("span.content-right_8Zs40") or item.select_one("div.c-abstract") or item.select_one("p")

            if a:
                title = a.get_text(strip=True)
                href = a.get("href", "")
                body = snippet.get_text(strip=True) if snippet else ""
                results.append({"title": title, "href": href, "body": body})

            if len(results) >= max_results:
                break

        if not results:
            return None

        formatted = ""
        for i, r in enumerate(results):
            formatted += f"{i+1}. [{r['title']}]({r['href']})\n   {r['body']}\n\n"
        return formatted

    except Exception as e:
        logger.warning(f"[WebTools] Baidu search failed: {e}")
        return None


def _ddg_search(query: str, max_results: int = 5) -> str:
    """ä½¿ç”¨ DuckDuckGo æœç´¢ (éœ€è¦ç¿»å¢™)"""
    if not DDGS_AVAILABLE:
        return None
    try:
        results = DDGS().text(query, max_results=max_results)
        if not results:
            return None

        formatted = ""
        for i, r in enumerate(results):
            formatted += f"{i+1}. [{r['title']}]({r['href']})\n   {r['body']}\n\n"
        return formatted
    except Exception as e:
        logger.warning(f"[WebTools] DuckDuckGo search failed: {e}")
        return None


@ToolRegistry.register(
    name="web_search",
    description="[å›½å†…å¯ç”¨] ç½‘ç»œæœç´¢ã€‚ä¼˜å…ˆæœç‹—ï¼Œå¤‡é€‰ç™¾åº¦/DuckDuckGoã€‚",
    tier=ToolTier.SLOW,
    schema={
        "type": "object",
        "properties": {
            "query": {"type": "string", "description": "æœç´¢å…³é”®è¯"},
            "max_results": {"type": "integer", "description": "æœ€å¤§ç»“æœæ•°é‡ (é»˜è®¤ 5)", "default": 5}
        },
        "required": ["query"]
    }
)
def web_search(query: str, max_results: int = 5):
    """
    æ™ºèƒ½æœç´¢ï¼šæœç‹— â†’ ç™¾åº¦ â†’ DuckDuckGo é€çº§é™çº§ã€‚
    """
    logger.info(f"[WebTools] Searching for: {query} (Max: {max_results})")

    # ç­–ç•¥: æœç‹—ä¼˜å…ˆ â†’ ç™¾åº¦å¤‡é€‰ â†’ DuckDuckGo æœ€å
    for name, fn in [("Sogou", _sogou_search), ("Baidu", _baidu_search), ("DuckDuckGo", _ddg_search)]:
        result = fn(query, max_results)
        if result:
            logger.info(f"[WebTools] âœ… {name} search succeeded.")
            return result
        logger.info(f"[WebTools] {name} unavailable, trying next...")

    return f"æœç´¢å¤±è´¥ï¼šæ‰€æœ‰æœç´¢å¼•æ“å‡æ— æ³•è·å–ç»“æœã€‚è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥ã€‚(Query: {query})"

import os
import hashlib
from datetime import datetime
from src.config.settings.settings import settings


def _run_coro_sync(coro, timeout: float = 60.0):
    """åœ¨åŒæ­¥å‡½æ•°ä¸­å®‰å…¨è¿è¡Œ coroutineã€‚

    - è‹¥å½“å‰çº¿ç¨‹æ²¡æœ‰è¿è¡Œä¸­çš„ event loopï¼šä½¿ç”¨ asyncio.run
    - è‹¥å·²æœ‰è¿è¡Œä¸­çš„ event loopï¼ˆå…¸å‹ï¼šWeb/Uvicorn ç¯å¢ƒï¼‰ï¼šåœ¨æ–°çº¿ç¨‹ä¸­å¯åŠ¨ç‹¬ç«‹ loop æ‰§è¡Œ

    è¿”å› coroutine çš„ç»“æœï¼›å¼‚å¸¸ä¼šå‘å¤–æŠ›å‡ºã€‚
    """
    try:
        asyncio.get_running_loop()
    except RuntimeError:
        return asyncio.run(coro)

    import threading

    result_container = {"result": None, "error": None}

    def runner():
        try:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            result_container["result"] = loop.run_until_complete(coro)
        except Exception as e:
            result_container["error"] = e
        finally:
            try:
                loop.close()
            except Exception:
                pass

    t = threading.Thread(target=runner, daemon=True)
    t.start()
    t.join(timeout=timeout)

    if t.is_alive():
        raise TimeoutError(f"Coroutine did not finish within {timeout}s")
    if result_container["error"] is not None:
        raise result_container["error"]

    return result_container["result"]


@ToolRegistry.register(
    name="web_crawl",
    description="[å›½å†…å¯ç”¨/æœ¬åœ°ç‰ˆ] ä½¿ç”¨ Crawl4AI (Playwright) æŠ“å–ç½‘é¡µå†…å®¹ã€‚è‡ªåŠ¨ä¿å­˜åˆ°çŸ¥è¯†åº“ä¸´æ—¶åŒºã€‚é€‚åˆè¯»å–é•¿æ–‡ç« ã€‚",
    tier=ToolTier.SLOW,
    schema={
        "type": "object",
        "properties": {
            "url": {"type": "string", "description": "è¦æŠ“å–çš„ç½‘é¡µ URL"},
            "bypass_cache": {"type": "boolean", "description": "æ˜¯å¦å¼ºåˆ¶åˆ·æ–°ç¼“å­˜ (é»˜è®¤ False)", "default": False}
        },
        "required": ["url"]
    }
)
def web_crawl(url: str, bypass_cache: bool = False):
    """
    åŒæ­¥åŒ…è£…å™¨ï¼Œè°ƒç”¨å¼‚æ­¥çˆ¬è™«ï¼Œå¹¶å°†ç»“æœä¿å­˜åˆ°æ–‡ä»¶
    """
    if not CRAWL4AI_AVAILABLE:
        return "Error: ç¼ºå°‘ä¾èµ– 'crawl4ai'ã€‚è¯·è¿è¡Œ `pip install crawl4ai` å’Œ `playwright install`ã€‚"

    # [Fix] URL æ¸…æ´—é€»è¾‘
    # ç§»é™¤å¯èƒ½çš„ Markdown é“¾æ¥æ ¼å¼ (e.g. [title](url))
    url = url.strip()
    if url.startswith("[") and "](" in url and url.endswith(")"):
        # æå–æ‹¬å·å†…çš„ URL
        try:
            url = url.split("](")[1][:-1]
        except:
            pass
            
    # ç¡®ä¿ URL ä»¥ http å¼€å¤´
    if not url.startswith("http"):
        # ç®€å•çš„å®¹é”™
        if url.startswith("www"):
            url = "https://" + url
        else:
             return f"Error: Invalid URL format: {url}. URL must start with http:// or https://"

    async def _crawl():
        async with AsyncWebCrawler(verbose=True) as crawler:
            result = await crawler.arun(url=url, bypass_cache=bypass_cache)
            if result.success:
                return result.markdown
            else:
                return f"Error: {result.error_message}"

    try:
        logger.info(f"[WebTools] Crawling URL: {url} (Bypass Cache: {bypass_cache})")
        content = _run_coro_sync(_crawl(), timeout=60)
        if content.startswith("Error"):
            logger.error(f"[WebTools] Crawl internal error: {content}")
            return content
            
        # [New] æ–‡æ¡£è½åœ° (Staging)
        # å°†æŠ“å–çš„å†…å®¹ä¿å­˜åˆ° e:\xingchen-V\storage\knowledge_staging
        staging_dir = os.path.join(settings.PROJECT_ROOT, "storage", "knowledge_staging")
        os.makedirs(staging_dir, exist_ok=True)
        
        # ç”Ÿæˆæ–‡ä»¶å (Hash + Timestamp)
        url_hash = hashlib.md5(url.encode()).hexdigest()[:8]
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"crawl_{timestamp}_{url_hash}.md"
        filepath = os.path.join(staging_dir, filename)
        
        # å†™å…¥æ–‡ä»¶
        with open(filepath, "w", encoding="utf-8") as f:
            f.write(f"# Source: {url}\n")
            f.write(f"# Date: {timestamp}\n\n")
            f.write(content)
            
        logger.info(f"[WebTools] Crawl successful. Saved to {filepath} ({len(content)} chars).")
        
        # è¿”å›æ‘˜è¦å’Œè·¯å¾„
        summary = content[:500] + "..." if len(content) > 500 else content
        return (
            f"âœ… æŠ“å–æˆåŠŸã€‚\n"
            f"ğŸ“ åŸå§‹æ–‡æ¡£å·²ä¿å­˜è‡³: {filepath}\n"
            f"ğŸ“„ å†…å®¹æ‘˜è¦ (å‰500å­—):\n"
            f"{summary}\n\n"
            f"(S-Brain è¯·æ³¨æ„ï¼šå®Œæ•´å†…å®¹å·²å½’æ¡£ï¼Œè¯·åœ¨åæ€å‘¨æœŸä¸­è¯»å–æ­¤æ–‡ä»¶è¿›è¡Œå†…åŒ–ã€‚)"
        )
            
    except Exception as e:
        logger.error(f"[WebTools] Crawl failed: {e}", exc_info=True)
        return f"Exception: {str(e)}"
